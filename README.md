# Interactive PDF Summarization & Q&A with AWS Bedrock (RAG with FAISS + LangChain)

## ğŸ“– Overview
This app lets you **chat with your PDFs** and get detailed summaries/answers using
foundation models on **Amazon Bedrock**. It builds a Retrieval-Augmented Generation (RAG)
pipeline with **Titan Embeddings** for vectorization, **FAISS** for similarity search,
and **Llama 3 / Claude 3** (via Bedrock) for generation. A **Streamlit** UI ties it
all together.

There are two variants provided (you can keep one or both):
1. Chat with Screenplays (folder: `screenplays`, FAISS index: `faiss_index_screenplays`)
2. Chat with PDFs (folder: `data`, FAISS index: `faiss_index`)

---

## âœ… Prerequisites
- Python 3.10+ recommended  
- An AWS account with access to **Amazon Bedrock** in a supported region  
- AWS CLI installed (`aws --version` to verify)  
- The following Bedrock models access requested:  
  - Embeddings: `amazon.titan-embed-text-v2:0`  
  - LLM (primary): `meta.llama3-70b-instruct-v1:0`  
  - LLM (optional/fast): `anthropic.claude-3-haiku-20240307-v1:0`  

> âš ï¸ **Note:** Amazon Bedrock is region-specific. Ensure your AWS region supports both Titan Embeddings and your chosen LLMs.

---

## ğŸ”‘ Step 1 â€” Create an IAM user with programmatic access
1. Go to **AWS Console â†’ IAM â†’ Users â†’ â€œCreate userâ€**  
2. Name: `bedrock-rag-user` (or any name)  
3. Access type: **Programmatic access**  
4. Attach a custom inline policy like:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "bedrock:InvokeModel",
        "bedrock:InvokeModelWithResponseStream",
        "bedrock:ListFoundationModels"
      ],
      "Resource": "*"
    }
  ]
}
```

5. Save the **Access Key ID** and **Secret Access Key**.

---

## âš™ï¸ Step 2 â€” Configure AWS CLI
Run:
```bash
aws configure
```
Provide:  
- AWS Access Key ID  
- AWS Secret Access Key  
- Default region (e.g., `us-east-1`)  
- Output format: `json`  

---

## ğŸ“¥ Step 3 â€” Request Model Access in Bedrock
1. Go to **AWS Console â†’ Amazon Bedrock â†’ Model access**  
2. Request access for:  
   - `amazon.titan-embed-text-v2:0`  
   - `meta.llama3-70b-instruct-v1:0`  
   - `anthropic.claude-3-haiku-20240307-v1:0` (optional)  
3. Wait until status = **Access granted**.  

---

## ğŸ“‚ Step 4 â€” Project Structure
```
.
â”œâ”€â”€ screenplay_rag.py      # App for screenplays (uses ./screenplays)
â”œâ”€â”€ app.py              # App for general PDFs (uses ./data)
â”œâ”€â”€ screenplays/             # Put screenplay PDFs here
â”œâ”€â”€ data/                    # Put other PDFs here
â””â”€â”€ README.txt               # This file
```

---

## ğŸ Step 5 â€” Setup Virtual Environment
```bash
python3 -m venv .venv
source .venv/bin/activate   # macOS/Linux
# OR
.venv\Scripts\activate    # Windows
```

---

## ğŸ“¦ Step 6 â€” Install Dependencies
```bash
pip install --upgrade pip
pip install boto3 streamlit langchain langchain-community faiss-cpu pypdf numpy
```

---

## ğŸ“‘ Step 7 â€” Place Your PDFs
- For **screenplay app** â†’ place files in `./screenplays`  
- For **generic PDF app** â†’ place files in `./data`  

The app will build a FAISS index after clicking **Vectors Update** in the sidebar.

---

## â–¶ï¸ Step 8 â€” Run the Apps
```bash
# Screenplays Q&A app
streamlit run screenplay_rag.py

# General PDF Q&A app
streamlit run app.py
```

In the Streamlit UI:
- Enter your question in the text box  
- Click **Vectors Update** once to build FAISS index  
- Click **Llama2 Output** (Llama 3 in code) to generate answers  

---

## ğŸ› ï¸ Troubleshooting
- **AccessDeniedException** â†’ Ensure model access is granted in the same region as `aws configure`.  
- **FAISS warnings** â†’ Keep `allow_dangerous_deserialization=True` when loading indexes.  
- **Apple Silicon issues** â†’ Pin FAISS version (`pip install faiss-cpu==1.8.0`).  

---

## ğŸ“Œ Tech Stack
- **AWS Bedrock** (Titan Embeddings, Llama 3, Claude 3)  
- **LangChain** (RetrievalQA, loaders, embeddings)  
- **FAISS** (vector database)  
- **Streamlit** (UI frontend)  

---

## ğŸ“œ License
MIT (or your preferred license)
